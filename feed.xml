<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://0xreentrant.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://0xreentrant.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-13T00:08:00+00:00</updated><id>https://0xreentrant.github.io/feed.xml</id><title type="html">reentrant</title><subtitle>security engineer, web3 protocols, and blockchain </subtitle><entry><title type="html">Testing the zkSync Era bootloader</title><link href="https://0xreentrant.github.io/blog/2024/testing-zksync-bootloader/" rel="alternate" type="text/html" title="Testing the zkSync Era bootloader"/><published>2024-02-08T00:00:00+00:00</published><updated>2024-02-08T00:00:00+00:00</updated><id>https://0xreentrant.github.io/blog/2024/testing-zksync-bootloader</id><content type="html" xml:base="https://0xreentrant.github.io/blog/2024/testing-zksync-bootloader/"><![CDATA[<blockquote> <p>NOTE: This was performed during the the last week of the Code4rena contest. While the time constraints prevented us from fully completing the endeavor, we plan to follow up post-contest.</p> </blockquote> <p>Remediations are included at the start of this document for pragmatic readers, and what follows is an analysis of the tooling available for the bootloader to be tested during an audit.</p> <p>To motivate this exploration, testing the bootloader was done with the intent of embodying the persona of a new hire attempting to build a testing suite for a niche aspect of the zkSync system. While the original intention was to build a fully functional test suite, the time allotted was too short to fully implement this vision.</p> <h2 id="remediations">Remediations</h2> <p>At the end of all this time, several suggestions were considered for the Matter Labs team to address:</p> <ol> <li>Provide an accessible, easy-to-update test suite, for the bootloader, to exercise both typical and edge-case transactions</li> <li>Add easy-to-access logging capability for further development, and auditing, of the bootloader</li> <li>Allow for accessible exploration of the bootloader memory throughout the execution of the bootloader</li> </ol> <h3 id="possible-test-suite">Possible test suite</h3> <p>For testing the bootloader, many kinds of scenarios can be formulated. The general structure of the scenarios is the execution of batches. In the the case that a batch contains a multitude of transactions, there is possibility of the state machine - that is implied by the bootloader’s code graph and memory - could provide opportunities for the invariants of the system to be violated.</p> <p>It would be valuable to create tests that exercise the bootloader in manners that demonstrate the invariants hold. High-level descriptions of invariants to test against attacks would be:</p> <ul> <li>escalate transaction type to privileged upgrade</li> <li>premature batch closure</li> <li>skipped transactions</li> <li>update batch data after closing</li> <li>incorrect gas calculations</li> <li>wrong tx.origin, msg.sender generated</li> <li>priorityQueue reordering</li> <li>general overwrite transaction data within batch</li> </ul> <h3 id="logging-capability">Logging capability</h3> <p>Put simply, providing easily accessible logging to the system through the test node would make exploration, development, and refactoring of the bootloader code a smoother process.</p> <h3 id="exploring-bootloader-memory">Exploring bootloader memory</h3> <p>Being able to peek into the memory of a running bootloader would be a powerful device. Possible methods could be - with no attempt to rank how difficult it would be to implement:</p> <ol> <li>Binary memory dumps: dump memory to files during execution so that a hex editor can be used to explore the data. Possible options could be to dump before or after each transaction, after a timeout, at every state change, etc.</li> <li>Provide a hook/file/endpoint for an external debugger to connect to: the canonical example is the Chrome webtools, which famously can connect to an external host to debug process on other devices. Likely there could be challenges in the control of the test node, pausing and resuming operation</li> <li>Include an internal debugger</li> </ol> <h1 id="analyzing-bootloader-tooling">Analyzing bootloader tooling</h1> <h2 id="assessing-testing-criteria">Assessing testing criteria</h2> <p>A survey of the provided codebase was made for any files related to the operation of the bootloader internals. No testing suites were found to satisfy that criteria.</p> <p>While examining the code of the bootloader itself, it was clear that the structure of this subsystem was unsuited to run-of-the-mill testing. This typically involves interacting with explicit interfaces defined in the code, including externally visible functions and constructors. What was found is that the bootloader operates on memory loaded by the zkSync Era EVM (which we’ll call “zkEVM” at times throughout this document). Additionally, there were no externally-visible functions, and no constructor.</p> <blockquote> <p>NOTE: In the scope of the zkSync Era codebase, it is clear that the bootloader was coded in Yul to take advantage of the increased control over the resulting compiled code’s performance. The consequence here is that, for Yul, the limited tooling in the wider EVM-based ecosystem leaves a lot to be desired in terms of developer experience, including the ability to test and benchmark Yul codebases.</p> </blockquote> <p>At this point, there were two major and relatively obvious paths that could be taken:</p> <ol> <li>Existing tools could be used to load and manipulate the bootloader memory and its environment</li> <li>The bootloader could be modified to provide external interfaces and a constructor to be used in popular testing frameworks</li> </ol> <p>It was decided that controlling the execution environment would be the best bet. The main factors were:</p> <ul> <li>controlling the memory to the granularity required by the bootloader would require brittle and difficult-to-maintain, parallel data structures between the zkEVM and test suite</li> <li>modifying the code changed the attack surface compared to the original implementation</li> <li>any modifications would likely change the memory outlay and performance characteristics of the original implementation <h3 id="criteria-for-testing-the-bootloader">Criteria for testing the bootloader</h3> </li> </ul> <p>During this time, the bootloader code itself was being analyzed for possible attack vectors. As a result, a handful of criteria were surfaced that could help explore these attacks:</p> <ol> <li>Many transactions would need to be available to load into memory to explore execution paths in the case memory segregation was compromised</li> <li>Full control over each transaction was necessary to explore cases where invariants across the zkEVM and bootloader execution were violated</li> <li>Logging and exploring the memory space was key <h2 id="exploring-tooling-options">Exploring tooling options</h2> </li> </ol> <p>Initially the in-scope, zkSync Era EVM code was explored to be used for loading the bootloader and constructing its memory. The understanding was that the zkEVM already had data structures and interfaces available that would make loading arbitrary transactions into the bootloader’s memory simple.</p> <p>While the code was available to explore, it was determined that the necessary “plumbing” to quickly get a proof-of-concept would take more time to understand and modify than was available. After discussing possibilities with the C4 sponsors, it was discovered that there was in fact a “test node” in development (era_test_node) that could provide a minimal environment for running the bootloader against individual transactions. This was quickly determined to be useful, and simple entry points and data structures were identified for controlling the bootloader memory:</p> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// matter-labs/era-test-node, src/node.rs</span>
<span class="mi">324</span><span class="p">:</span>    <span class="k">if</span> <span class="o">!</span><span class="n">transactions_to_replay</span><span class="nf">.is_empty</span><span class="p">()</span> <span class="p">{</span>
<span class="mi">325</span><span class="p">:</span>        <span class="k">let</span> <span class="n">_</span> <span class="o">=</span> <span class="n">node</span><span class="nf">.apply_txs</span><span class="p">(</span><span class="n">transactions_to_replay</span><span class="p">);</span>
<span class="mi">326</span><span class="p">:</span>    <span class="p">}</span>
</code></pre></div></div> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// matter-labs/era-test-node, src/node.rs</span>
<span class="mi">1204</span><span class="p">:</span> <span class="k">pub</span> <span class="k">fn</span> <span class="nf">run_l2_tx_inner</span><span class="p">(</span>
<span class="o">...</span> 
<span class="mi">1245</span><span class="p">:</span>         <span class="k">let</span> <span class="n">tx</span><span class="p">:</span> <span class="n">Transaction</span> <span class="o">=</span> <span class="n">l2_tx</span><span class="nf">.clone</span><span class="p">()</span><span class="nf">.into</span><span class="p">();</span>
<span class="mi">1246</span><span class="p">:</span> 
<span class="mi">1247</span><span class="p">:</span>         <span class="n">vm</span><span class="nf">.push_transaction</span><span class="p">(</span><span class="n">tx</span><span class="p">);</span>
</code></pre></div></div> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// matter-labs/zksync-era, core/lib/types/src/l2/mod.rs</span>
<span class="mi">133</span><span class="p">:</span> <span class="k">pub</span> <span class="k">struct</span> <span class="n">L2Tx</span> <span class="p">{</span>
</code></pre></div></div> <h3 id="roadblocks---single-transaction-mode">Roadblocks - single transaction mode</h3> <p>During discussions with one of the sponsors, it was made clear that the zkEVM used with the test node was configured to only run the bootloader with a single transaction at a time. This would be a roadblock. Scanning the code, it was determined that the solution was to include an existing VM setting:</p> <div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// matter-labs/zksync-era, core/lib/multivm/src/interface/types/inputs/execution_mode.rs</span>
<span class="mi">8</span><span class="p">:</span>    <span class="nn">vm</span><span class="p">::</span><span class="nn">VmExecutionMode</span><span class="p">::</span><span class="n">Bootloader</span>
</code></pre></div></div> <h3 id="logging-woes">Logging woes</h3> <p>During the implementation of a rough proof-of-concept, it was made clear that there was no easy way to read the logs from the bootloader through the test node. The two suggestions from a sponsor were:</p> <ol> <li>Use the <code class="language-plaintext highlighter-rouge">log0</code> - <code class="language-plaintext highlighter-rouge">log4</code> events in Yul so logs could be caught by the test node.</li> <li>Send transactions to the <code class="language-plaintext highlighter-rouge">Console</code> system contract.</li> </ol> <p>For the first option, the output would be a series of hex-encoded values according to the event parameters. For the second option, the test node would output the logged strings using when run with the correct command-line setting. This would be the preferred option.</p> <p>Unfortunately, neither seemed to be a great option, since the existing <code class="language-plaintext highlighter-rouge">debugLog</code> calls in the existing bootloader would not work. A third option was to be forking the <code class="language-plaintext highlighter-rouge">zksync-era</code> crate that was imported into the project and adding <code class="language-plaintext highlighter-rouge">println!</code> calls for when the <code class="language-plaintext highlighter-rouge">debugLog</code> hook was triggered.</p> <h3 id="results">Results</h3> <p>While there wasn’t time to fully implement the plan, the intended plan of attack became:</p> <ol> <li>Add a command line setting through <code class="language-plaintext highlighter-rouge">clap</code> to support loading a JSON file with multiple transactions and their fields</li> <li>Parse the JSON with <code class="language-plaintext highlighter-rouge">serde</code> and load the data into <code class="language-plaintext highlighter-rouge">L2Tx</code> instances</li> <li>Fill <code class="language-plaintext highlighter-rouge">transactions_to_replay</code> with the data</li> <li>Switch the vm execution mode to <code class="language-plaintext highlighter-rouge">Bootloader</code> while this new command line setting is triggered</li> <li>Fork and update the <code class="language-plaintext highlighter-rouge">zksync-era</code> crate to output logs in the <code class="language-plaintext highlighter-rouge">debugLog</code> hook</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[What does it take to test the security of a blockchain's most important core smart-contract?]]></summary></entry><entry><title type="html">Don’t let mocks in your protocol tests fool you</title><link href="https://0xreentrant.github.io/blog/2023/mocks-fool-you/" rel="alternate" type="text/html" title="Don’t let mocks in your protocol tests fool you"/><published>2023-10-19T00:00:00+00:00</published><updated>2023-10-19T00:00:00+00:00</updated><id>https://0xreentrant.github.io/blog/2023/mocks-fool-you</id><content type="html" xml:base="https://0xreentrant.github.io/blog/2023/mocks-fool-you/"><![CDATA[<h1 id="dont-let-mocks-in-your-protocol-tests-fool-you">Don’t let mocks in your protocol tests fool you</h1> <blockquote> <p>Note: This article was written for web3 protocol developers. Security researchers will also get a little glimpse into techniques for testing protocols.</p> </blockquote> <p><em>Mocking expensive, slow, or complex-to-set-up components is a great way to make testing faster, cheaper, and simpler to leverage. What can happen though, is that the expectations of the mock can diverge from the expectations of the real-world object. When that happens, you can be oblivious to the fact your codebase does not, in fact, work, green checkmarks be damned.</em></p> <p><em>The biggest threat to developing a protocol is the mental model of the protocol within the mind of the developer. A flawed mental model happens from numerous sources, but today we’ll focus on a single one: the feeling of security that comes with passing tests</em></p> <blockquote> <p>Note: I’m using the definition of “mock” to be along the lines of how Martin Fowler uses it here: https://martinfowler.com/articles/mocksArentStubs.html</p> </blockquote> <p><strong>TL;DR</strong> If you want to skip to what kind of things you can do to prevent mocks from fooling you, jump to <a href="#how-can-I-ensure-our-mocks-are-correct">How can I ensure our mocks are correct?</a></p> <h2 id="our-case">Our case</h2> <p>Imagine getting green checkmarks on all your tests. You’ve gone ahead and built a suite to exercise the entire happy path of your codebase, and even some edge cases to boot. And even better: every test passed. It’s a good time to grab a beer or wine and take a little break, because next you’re going to discover something unsavory when you try to run through the deployed example.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/all-tests-green-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/all-tests-green-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/all-tests-green-1400.webp"/> <img src="/assets/img/all-tests-green.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <blockquote> <p>Note: Before you hire a firm or to audit your code, do make sure to fully deploy your code to a testnet with all 3rd-party contracts and features available. For example: testing cross-chain protocols</p> </blockquote> <p>Cutting to the chase, you’re going to “Oh no, what’s wrong, I coded this <em>perfectly</em> - all the tests pass!” And you’re going to feel so right, because your mental model was impeccable and your tests reflected that… <em>but your mocks fooled you</em>. Actually, it’s just a case of a “<a href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">false positive</a>” (lest we lose our last shred of sanity and <a href="https://psychcentral.com/health/why-do-we-anthropomorphize">anthropomorphize</a> our code). Green checks that don’t give confidence are the antithesis of any testing methodology, so let’s break down what to keep in mind.</p> <h2 id="mocks-have-tradeoffs">Mocks have tradeoffs</h2> <p>When we use mocks, we’re taking a stance. The two main ones I hear and have held are:</p> <ol> <li>Our dependencies are too expensive, slow, or complex to spin up each time we test (<em>fair!</em>)</li> <li>We want our tests to run quickly, and we can’t wait for each test iteration to fully pass or well lose out on momentum (<em>fair!</em>)</li> </ol> <h3 id="but-what-are-the-consequences">But what are the consequences?</h3> <p>These tradeoffs, again, are fair to make when the stakes are low, like:</p> <ul> <li>When you’re building the protocol, pre-launch</li> <li>When you’re adding to an existing protocol and need the flexibility to refactor highly-coupled parts of a system</li> <li>When you’re exploring how to implement a new system</li> </ul> <p>Etc.</p> <p>But in the case of running a web3, immutable, smart-contract-based, financial protocol the stakes are high. As protocol developers in a hyper-competitive environment with close to zero consequences for exploitation by psuedo-anonymous actors that include globally-sanctioned blackhats, more care must be taken before accepting real people’s assets. I’ll let our industry’s favorite rug-pull <a href="https://rekt.news/stake-rekt/">journalist</a> say it best:</p> <p>They say the house always wins. Not in crypto.</p> <h2 id="how-can-i-ensure-our-mocks-are-correct">How can I ensure our mocks are correct?</h2> <p>“But reentrant, we read the documentation front-to-back and everything is working just like they said!”</p> <p>I 100% get it: you’ve put in the work.</p> <p>But, what if:</p> <ul> <li>The documentation is wrong</li> <li>The implementation has changed</li> <li>You used the wrong flag for a key function while trying to get that last PR in at 3am Saturday morning</li> </ul> <p>Then things break.</p> <p>What do you do? Add <em>at least</em> one more step, especially before deploying your new protocol.</p> <ol> <li>Create “smoke-tests” - as in short, typically disposable scripts that call major 3rd-party components the same as the mocks would be called.</li> <li>Deploy on a fully-functional testnet and run manual tests for all major features.</li> <li>Build “forking tests” that fork a live network locally and test against it. Some dependencies may not work if they are off-chain.</li> <li>Build integration tests for all major features, deploy on a fully-functional testnet, and test against it.</li> </ol> <p>&lt; decision tree for determinnig when mocks fail &gt;</p> <h2 id="what-do-these-possibilities-look-like">What do these possibilities look like?</h2> <p>There’s nothing like a real environment to expose the brittleness of a test suite’s assumptions. While it might seem unnecessary “because it’s there in the code,” the unique case of web3 warrants the extra paranoia.</p> <h3 id="smoke-tests">Smoke tests</h3> <p>This one makes sense when your contracts interface with 3rd-party code. Each feature that touches a 3rd-party contract is either triggered in a script or run manually. If the feature breaks, but your mocking tests were all passing, there is an issue with your mock.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/smoke-tests-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/smoke-tests-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/smoke-tests-1400.webp"/> <img src="/assets/img/smoke-tests.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>For example: imagine you are a protocol developer lead that wants to test that each feature that is mocked works. You would:</p> <ol> <li>Identify all the features that touch 3rd-party code.</li> <li>Write a script to exercise the 3rd-party contract code for each relevant feature, according to how you expect the API of that code to work.</li> <li>Run your script.</li> <li>Make a note if something broke, and how it broke</li> </ol> <p>You will find that increased logging through events or console logs will be useful when creating your scripts.</p> <p>We won’t go much deeper on the subject of smoke tests in web3, but here’s a great article for learning more about smoke tests: https://www.edureka.co/blog/what-is-smoke-testing/</p> <h3 id="manual-testing-on-a-testnet">Manual testing on a testnet</h3> <p>This one is pretty obvious in operation. The goal here is to make sure every feature that interfaces with 3rd-party components has been run, along with some edge-cases that have been identified. That way you are exercising whatever assumptions your code is making about the interface of code you don’t control.</p> <p>The advantage to running these on an existing testnet is that any off-chain systems your preferred 3rd-party system uses will be more or less accessible. A downside would be the necessity of amassing gas tokens for the respective testnet.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/manual-tests-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/manual-tests-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/manual-tests-1400.webp"/> <img src="/assets/img/manual-tests.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Similarly to running smoke tests, the point is to get an actual look at whether any assumptions encoded in the codebase, and your mocks, match the real-world requirements of any 3rd-party component.</p> <p>Doing this yourself:</p> <ol> <li>Identify all the features that touch 3rd-party code.</li> <li>Perform all the operations in your app to exercise the 3rd-party contract code for each relevant feature, according to how you expect the API of that code to work.</li> <li>Make a note if something broke, and how it broke</li> </ol> <p>If any of your mocking tests passed, but manual testing fails where your tests had mocks, then that’s an unmistakable sign something is broken in your codebase <em>and</em> tests.</p> <h3 id="forking-tests">Forking tests</h3> <p>When there is no need to monitor the results of any off-chain systems, you can fork any live chain and run a version of it locally for more control and execution speed. This also is advantageous for the off-chance that your 3rd-party of choice decides not to run a version on a testnet.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/forking-tests-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/forking-tests-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/forking-tests-1400.webp"/> <img src="/assets/img/forking-tests.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Unlike the smoke tests and the manual tests, we are back to writing automated tests. We can use our existing testing suites, with the modification that now any mocked calls will instead call actual contracts hosted on the forked testnet. That way any assumptions that mocks encode will be instead tested against actual code.</p> <p>A disadvantage of this approach is that the tests themselves will likely take longer to run each cycle compared to the mocking tests. However, compared to smoke tests and manual testing, you’ll get the increased coverage and repeatability of automated testing along with the better assurance of correctness.</p> <h3 id="live-integration-tests">Live Integration Tests</h3> <p>The most intensive version of testing that would shake out any assumptions about 3rd-party interfaces in your codebase would be integration tests on a live testnet. All components are deployed to the testnet, mocks in tests are replaced with calls to actual on-chain resources, all tests run against the live testnet, and off-chain resources are available to monitor. You will need any testnet gas tokens for every run of the test suite.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/integration-tests-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/integration-tests-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/integration-tests-1400.webp"/> <img src="/assets/img/integration-tests.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Every test will exercise live code as close to the final, real-world software environment possible. Any tests that fail here will give a lot of information about assumptions encoded in your codebase. Because there will be subtle differences between a local fork and an actual live chain, and the cost of running the test suite (in time and testnet gas) it will be helpful to monitor the results of the tests with other tools such as chain explorers like <a href="https://tenderly.co/">Tenderly</a> for delving into any edge cases that the other techniques did not uncover.</p> <h2 id="conclusion">Conclusion</h2> <p>We can see that mocks are helpful, but they have tradeoffs. To counter the possibility that mocks will give us false-positives, we can increase the assurances by running tests against actual 3rd-party code.</p> <p>In web3, it is imperative that all precautions are made to ensure real money isn’t lost to thefts or mishandling of digital currency. Test more, test often, and test comprehensively.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The biggest threat to developing a protocol is the mental model]]></summary></entry></feed>